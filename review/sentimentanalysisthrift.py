# -*- coding: utf-8 -*-
"""SentimentAnalysisThrift.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gg9RkQHUc6DptZdgTQuPpKfE3dPJtKYQ
"""

# Load libraries
import io
import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import pickle

stop_words = stopwords.words('english')
clothes =['dress', 'color', 'wear', 'top', 'sweater', 'material', 'shirt', 'jeans', 'pant',
          'skirt', 'order', 'white', 'black', 'fabric', 'blouse', 'sleeve', 'even', 'jacket']
lem = WordNetLemmatizer()


def clean_text(words):
    """The function to clean text"""
    words = re.sub("[^a-zA-Z]", " ", words)
    text = words.lower().split()
    return " ".join(text)


def remove_stopwords(review):
    """The function to removing stopwords"""
    text = [word.lower() for word in review.split() if word.lower() not in stop_words and word.lower() not in clothes]
    return " ".join(text)


def remove_numbers(text):
    """The function to removing all numbers"""
    new_text = []
    for word in text.split():
        if not re.search('\d', word):
            new_text.append(word)
    return ' '.join(new_text)


def get_lemmatize(text):
    """The function to apply lemmatizing"""
    lem_text = [lem.lemmatize(word) for word in text.split()]
    return " ".join(lem_text)



#data = dataset.to_csv('C:\\Python Scripts\\NLP_projekty\\review_clean.csv', encoding='utf-8')

# Load dataset
from google.colab import files
uploaded = files.upload()

dataset = pd.read_csv(io.BytesIO(uploaded['reviews.csv']), header=0, index_col=0)

# shape
print(dataset.shape)
print(dataset.head(5))

dataset['Review Text'] = dataset['Review Text'].astype(str)
dataset['Review Text'] = dataset['Review Text'].apply(clean_text)
dataset['Review Text'] = dataset['Review Text'].apply(remove_stopwords)
dataset['Review Text'] = dataset['Review Text'].apply(remove_numbers)
dataset['Review Text'] = dataset['Review Text'].apply(get_lemmatize)
print(dataset[:5])

with open('review_clean.csv', 'w') as f:
  dataset.to_csv(f)

dataset = pd.read_csv('review_clean.csv', header=0, index_col=0)

# Shape
print(dataset.shape)

# Separate into input and output columns
X = dataset['Review Text'].values.astype('U')
y = dataset['Recommended IND']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)
model = Pipeline([('vect', CountVectorizer(min_df=5, ngram_range=(1, 2))),
                  ('tfidf', TfidfTransformer()),
                  ('model', LogisticRegression()), ])

# Create Logistic regression model
model.fit(X_train, y_train)

# Make predictions
ytest = np.array(y_test)
pred_y = model.predict(X_test)

# Evaluate predictions
print('accuracy %s' % accuracy_score(pred_y, y_test))
print(classification_report(ytest, pred_y))

# Save the model
with open("model_file.pkl", "wb") as f:
    pickle.dump(model,f)

with open('model_file.pkl', 'rb') as f:
        model = pickle.load(f)
s=["pretty and ugly"]
a=model.predict(s)
print(a[0])